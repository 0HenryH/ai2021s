{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业2：纯天然手工制作神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业中，你将体验到如何用最基本的矩阵运算来实现一个前馈神经网络。在该实现中，唯一需要使用到的软件包是Numpy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 Git 的基本操作，对第一次作业进行一次修订。具体要求如下：\n",
    "\n",
    "1. 将库的名称统一为 “ai2021s”。\n",
    "2. 在库的根目录创建文件夹 `HW1`，将第一次作业提交的 Notebook 文件重命名为 `numpy_practice.ipynb`，并移至 `HW1` 文件夹中。\n",
    "3. 提交这一修改，提交信息为 “reorganizing project files”。\n",
    "4. 从本次作业起，均按上述规则建立文件夹。本次作业的文件名为`fnn.ipynb`。\n",
    "5. 优化作业1中各函数的编写，直接对原始文件修改并利用 Git 命令提交。根据本次修改的结果，你将对作业1获得最多20分的加分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 根据作业1的内容，编写**数值稳定**的函数 `sigmoid(x)` 和 `softplus(x)`，要求它适用于 Numpy 向量和矩阵，即当 `x` 是一维或二维数组时，返回一个相同大小和形状的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Sigmoid(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n",
      "Softplus(v) =\n",
      " [0.29094333 1.31132175 0.84461281 0.20026791 0.44512331 1.82687967]\n",
      "Softplus(m) =\n",
      " [[0.08464411 0.50151249 1.51433825]\n",
      " [0.35088177 0.41024142 0.6469135 ]\n",
      " [1.69437919 0.42387573 0.49559644]\n",
      " [0.49937109 2.31042345 2.29319537]\n",
      " [1.31622694 0.90476816 1.12830942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100) \n",
    "\n",
    "def sigmoid(x):\n",
    "    den =  1 + np.exp(-np.abs(x))\n",
    "    nom = np.where(x>0, 1.0, np.exp(x))\n",
    "    res = nom / den\n",
    "    return res\n",
    "\n",
    "def softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    p_2 = np.log(1.0 + np.exp(-np.abs(x)))\n",
    "    p_1 = deepcopy(x)\n",
    "    p_1[p_1<0] = 0.0\n",
    "    res = p_1 + p_2\n",
    "    return res\n",
    "\n",
    "np.random.seed(123)\n",
    "vec = np.random.normal(size=6)\n",
    "mat = np.random.normal(size=(5, 3))\n",
    "print(\"Sigmoid(v) =\\n\", sigmoid(vec))\n",
    "print(\"Sigmoid(m) =\\n\", sigmoid(mat))\n",
    "print(\"Softplus(v) =\\n\", softplus(vec))\n",
    "print(\"Softplus(m) =\\n\", softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 推导 Sigmoid 和 Softplus 函数的导数，编写**数值稳定**的函数 `d_sigmoid(x)` 和 `d_softplus(x)` 计算对应的导数，同样要求适用于向量和矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x)=1/(1+\\exp(-x)),\\ \\sigma'(x)=...$\n",
    "\n",
    "$\\mathrm{softplus}(x)=\\log(1+\\exp(x)),\\ \\mathrm{softplus}'(x)=...$\n",
    "\n",
    "注意到 Sigmoid 和 Softplus 之间的联系了吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid'(v) =\n",
      " [0.18871502 0.196853   0.24506124 0.14855047 0.23019077 0.13502129]\n",
      "Sigmoid'(m) =\n",
      " [[0.07457369 0.23884569 0.17157406]\n",
      " [0.20835666 0.223271   0.24944023]\n",
      " [0.14996269 0.22612814 0.23807373]\n",
      " [0.23856977 0.08937477 0.09075383]\n",
      " [0.19624332 0.24090565 0.21887592]]\n",
      "Softplus'(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Softplus'(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n"
     ]
    }
   ],
   "source": [
    "def d_sigmoid(x):\n",
    "    sigx = sigmoid(x)\n",
    "    res = sigx * (1.0 - sigx)\n",
    "    return res\n",
    "\n",
    "def d_softplus(x):\n",
    "    res = sigmoid(x)\n",
    "    return res\n",
    "\n",
    "print(\"Sigmoid'(v) =\\n\", d_sigmoid(vec))\n",
    "print(\"Sigmoid'(m) =\\n\", d_sigmoid(mat))\n",
    "print(\"Softplus'(v) =\\n\", d_softplus(vec))\n",
    "print(\"Softplus'(m) =\\n\", d_softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 编写函数 `relu(x)` 和 `d_relu(x)` 来实现 ReLU 函数及其导数，$\\mathrm{ReLU}(x)=\\max(x,0)$，要求适用于向量和矩阵。对于不可导的点，可以将导数取为左导数或右导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(v) =\n",
      " [0.         0.99734545 0.2829785  0.         0.         1.65143654]\n",
      "ReLU(m) =\n",
      " [[0.         0.         1.26593626]\n",
      " [0.         0.         0.        ]\n",
      " [1.49138963 0.         0.        ]\n",
      " [0.         2.20593008 2.18678609]\n",
      " [1.0040539  0.3861864  0.73736858]]\n",
      "ReLU'(v) =\n",
      " [0. 1. 1. 0. 0. 1.]\n",
      "ReLU'(m) =\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    res = deepcopy(x)\n",
    "    res[res<0] = 0.0\n",
    "    return res\n",
    "\n",
    "def d_relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res[x>0] = 1.0\n",
    "    return res\n",
    "\n",
    "print(\"ReLU(v) =\\n\", relu(vec))\n",
    "print(\"ReLU(m) =\\n\", relu(mat))\n",
    "print(\"ReLU'(v) =\\n\", d_relu(vec))\n",
    "print(\"ReLU'(m) =\\n\", d_relu(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习标量的反向传播算法。考虑一个两层的前馈神经网络（见第5周课件107页），其中数据和权重都是标量：\n",
    "\n",
    "![](img/model1.png)\n",
    "\n",
    "如下为正向的计算过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.2345\n",
    "w1 = 0.111\n",
    "b1 = 0.222\n",
    "w2 = 0.333\n",
    "b2 = -0.444\n",
    "\n",
    "a0 = x\n",
    "z1 = w1 * a0 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = z2\n",
    "\n",
    "yhat = a2\n",
    "y = 5.4321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令损失函数为 $l=(y-\\hat{y})^2$，则可以计算出 $l$ 对 $a_2=\\hat{y}$ 的导数为 $\\mathrm{d}l/\\mathrm{d}a_2=2(a_2-y)$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_l_a2 = 2.0 * (a2 - y) # This is the \"upstream derivative\" associated with a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 $a_2$ 的导数反向传播给 $z_2$。首先计算 $a_2$ 对 $z_2$ 的“局部导数”：因为 $a_2=z_2$，所以局部导数为1。然后将“上游导数”乘以“局部导数”，即得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_a2_z2 = 1.0 # The \"local derivative\" of a2 with respect to z2\n",
    "d_l_z2 = d_l_a2 * d_a2_z2 # The \"downstream derivative\" of l with respect to z2\n",
    "\n",
    "print(d_l_z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将 $z_2$ 的导数继续向下游传播。在上一层的计算中得到的“下游导数”现在变成了“上游导数”，进而向 $a_1$、$w_2$ 和 $b_2$ 传播。依然进行局部导数的计算，可以得到 $\\mathrm{d}z_2/\\mathrm{d}a_1=w_2$, $\\mathrm{d}z_2/\\mathrm{d}w_2=a_1$, $\\mathrm{d}z_2/\\mathrm{d}b_2=1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z2_a1 = w2\n",
    "d_z2_w2 = a1\n",
    "d_z2_b2 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是通过链式法则，将“上游导数”乘以“局部导数”得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7828984975710127\n",
      "-6.688862995036224\n",
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2 * d_z2_a1\n",
    "d_l_w2 = d_l_z2 * d_z2_w2\n",
    "d_l_b2 = d_l_z2 * d_z2_b2\n",
    "\n",
    "print(d_l_a1)\n",
    "print(d_l_w2)\n",
    "print(d_l_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}l/\\mathrm{d}w_1$ 和 $\\mathrm{d}l/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z2_z1 = w2 * d_sigmoid(z1)\n",
    "\n",
    "d_l_z1 = d_l_z2 * d_z2_z1\n",
    "\n",
    "d_z1_w1 = a0\n",
    "d_z1_b1 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1306675893376559\n",
      "-0.915891121375177\n"
     ]
    }
   ],
   "source": [
    "d_l_w1 = d_l_z1 * d_z1_w1\n",
    "d_l_b1 = d_l_z1 * d_z1_b1\n",
    "\n",
    "print(d_l_w1)\n",
    "print(d_l_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在考虑一个更实际的场景，首先，数据包含多个观测，每个观测占据一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出输入的维度为 $p=2$，输出的维度为 $d=1$，样本量为 $n=4$。我们将构建一个两层的前馈神经网络，其中隐藏层的维度为 $r=5$。计算流程如下：\n",
    "\n",
    "![](img/model2.png)\n",
    "\n",
    "$Z_1=XW_1+\\mathbf{1}_nb_1^T,\\quad A_1=\\mathrm{softplus}(Z_1)$\n",
    "\n",
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$\n",
    "\n",
    "其中 $\\mathbf{1}_n$ 为元素全为1的 $n\\times 1$ 向量，$W_1$ 为 $p\\times r$ 矩阵，$b_1$ 为 $r\\times 1$ 向量，$W_2$ 为 $r\\times d$ 矩阵，$b_2$ 为 $d\\times 1$ 向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "a0 = x\n",
    "ones = np.ones((n, 1))\n",
    "z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "a1 = softplus(z1)\n",
    "z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "phat = a2\n",
    "print(phat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义第 $i$ 个观测的损失函数为 $l(y_i,\\hat{p}_i)=-y_i \\cdot \\log(\\hat{p}_i)-(1-y_i) \\cdot \\log(1-\\hat{p}_i)$，请推导出 $l$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}l/\\mathrm{d}\\hat{p}_i=\\frac{\\hat{p}_i - y_i}{\\hat{p}_i(1 - \\hat{p}_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义整体的损失函数为 $L(y,\\hat{p})=n^{-1}\\sum_{i=1}^n l(y_i,p_i)$，请给出 $L$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}L/\\mathrm{d}\\hat{p}_i=\\frac{1}{n}\\frac{dl}{d\\hat{p}_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 $\\mathrm{d}L/\\mathrm{d}\\hat{p}=(\\mathrm{d}L/\\mathrm{d}\\hat{p}_1,\\ldots,\\mathrm{d}L/\\mathrm{d}\\hat{p}_n)^T$ 看作一个向量，计算出其结果并赋给变量 `d_l_a2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137.54924583]\n",
      " [ -0.25286142]\n",
      " [ -0.25009615]\n",
      " [ 29.03650883]]\n"
     ]
    }
   ],
   "source": [
    "d_l_a2 = ((phat - y) / (phat * (1.0 - phat))) / n\n",
    "print(d_l_a2)\n",
    "\n",
    "if d_l_a2.shape != a2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d_l_a2` 是损失函数对 $A_2$ 的“上游导数”，我们需要将其传播给下游 $Z_2$。由于 $A_2=\\mathrm{sigmoid}(Z_2)$，根据课件中反向传播的规则1，可以得到下游导数为\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}Z_2=\\mathrm{d}L/\\mathrm{d}A_2\\circ \\mathrm{sigmoid}'(Z_2)$，\n",
    "\n",
    "其中 $\\mathrm{sigmoid}'$ 是 Sigmoid 的导数，$\\circ$ 是向量或矩阵的逐元素相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.49545617e-01]\n",
      " [-2.82903496e-03]\n",
      " [-9.61178724e-05]\n",
      " [ 2.47847537e-01]]\n"
     ]
    }
   ],
   "source": [
    "d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "print(d_l_z2)\n",
    "\n",
    "if d_l_z2.shape != z2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，再将 $Z_2$ 的“上游导数”传给 $A_1$、$W_2$ 和 $b_2$。根据规则2，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}A_1=(\\mathrm{d}L/\\mathrm{d}Z_2)W_2^T$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}W_2=A_1^T(\\mathrm{d}L/\\mathrm{d}Z_2)$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}b_2=(\\mathrm{d}L/\\mathrm{d}Z_2)^T\\mathbf{1}_n$。\n",
    "\n",
    "请根据如上公式计算对应的导数，并赋值给相应的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_A1:\n",
      " [[-1.08390457e-01  5.50480184e-01  5.45702884e-01  2.50557250e-01  9.63711234e-02]\n",
      " [ 1.22879494e-03 -6.24065331e-03 -6.18649429e-03 -2.84050357e-03 -1.09253482e-03]\n",
      " [ 4.17489205e-05 -2.12029306e-04 -2.10189226e-04 -9.65075245e-05 -3.71194151e-05]\n",
      " [-1.07652894e-01  5.46734339e-01  5.41989547e-01  2.48852286e-01  9.57153480e-02]]\n",
      "d_W2:\n",
      " [[0.25687749]\n",
      " [0.20993967]\n",
      " [0.80962486]\n",
      " [0.18883191]\n",
      " [0.15786162]]\n",
      "d_b2:\n",
      " [[0.494468]]\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2.dot(w2.T)\n",
    "print(\"d_A1:\\n\", d_l_a1)\n",
    "\n",
    "d_l_w2 = a1.T.dot(d_l_z2)\n",
    "print(\"d_W2:\\n\", d_l_w2)\n",
    "\n",
    "d_l_b2 = d_l_z2.T.dot(ones)\n",
    "print(\"d_b2:\\n\", d_l_b2)\n",
    "\n",
    "if (d_l_a1.shape != a1.shape) | (d_l_w2.shape != w2.shape) | (d_l_b2.shape != b2.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}L/\\mathrm{d}W_1$ 和 $\\mathrm{d}L/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a1_z1 = d_softplus(z1)\n",
    "d_l_z1 = d_l_a1 * d_a1_z1\n",
    "\n",
    "d_z1_w1 = a0\n",
    "d_z1_b1 = ones\n",
    "\n",
    "d_l_w1 = d_z1_w1.T.dot(d_l_z1)\n",
    "d_l_b1 = d_l_z1.T.dot(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_W1:\n",
      " [[-0.05078023  0.09764186  0.42982702  0.07298599  0.01255969]\n",
      " [-0.04989469  0.09732858  0.42540916  0.07114462  0.01233749]]\n",
      "d_b1:\n",
      " [[-0.08636305]\n",
      " [ 0.3593937 ]\n",
      " [ 0.87067948]\n",
      " [ 0.15770073]\n",
      " [ 0.04998879]]\n"
     ]
    }
   ],
   "source": [
    "print(\"d_W1:\\n\", d_l_w1)\n",
    "\n",
    "print(\"d_b1:\\n\", d_l_b1)\n",
    "\n",
    "if (d_l_w1.shape != w1.shape) or (d_l_b1.shape != b1.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将第4题中的步骤封装成三个函数：`forward()` 计算正向传播结果，得到预测值向量 `phat`，`loss()` 计算整体损失函数值，`backward()` 计算反向传播后各参数的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, b1, w2, b2):\n",
    "    n = x.shape[0]\n",
    "    a0 = x\n",
    "    ones = np.ones((n, 1))\n",
    "    z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "    a1 = softplus(z1)\n",
    "    z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def loss_function(y, phat):\n",
    "    eloss = -y * np.log(phat) - (1.0 - y) * np.log(1 - phat)\n",
    "    loss = np.mean(eloss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y):\n",
    "    n = x.shape[0]\n",
    "    phat = a2\n",
    "    d_l_a2 = ((phat - y) / (phat * (1.0 - phat))) / n\n",
    "    d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "    \n",
    "    d_l_a1 = d_l_z2.dot(w2.T)\n",
    "    d_l_w2 = a1.T.dot(d_l_z2)\n",
    "    d_l_b2 = d_l_z2.T.dot(ones)\n",
    "    \n",
    "    d_a1_z1 = d_softplus(z1)\n",
    "    d_l_z1 = d_l_a1 * d_a1_z1\n",
    "\n",
    "    d_z1_w1 = a0\n",
    "    d_z1_b1 = ones\n",
    "\n",
    "    d_l_w1 = d_z1_w1.T.dot(d_l_z1)\n",
    "    d_l_b1 = d_l_z1.T.dot(ones)\n",
    "    \n",
    "    return d_l_w1, d_l_b1, d_l_w2, d_l_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的结果互相印证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phat =\n",
      " [[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n",
      "loss = 2.7692224724634995\n",
      "(dw1, db1, dw2, db2) =\n",
      " (array([[-0.05078023,  0.09764186,  0.42982702,  0.07298599,  0.01255969],\n",
      "       [-0.04989469,  0.09732858,  0.42540916,  0.07114462,  0.01233749]]), array([[-0.08636305],\n",
      "       [ 0.3593937 ],\n",
      "       [ 0.87067948],\n",
      "       [ 0.15770073],\n",
      "       [ 0.04998879]]), array([[0.25687749],\n",
      "       [0.20993967],\n",
      "       [0.80962486],\n",
      "       [0.18883191],\n",
      "       [0.15786162]]), array([[0.494468]]))\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "phat = a2\n",
    "loss = loss_function(y, phat)\n",
    "grads = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "\n",
    "print(\"phat =\\n\", phat)\n",
    "print(\"loss =\", loss)\n",
    "print(\"(dw1, db1, dw2, db2) =\\n\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后将这三步放在循环中，并利用梯度下降法拟合出 $x$ 和 $y$ 之间的函数关系。下面是一个完整的神经网络模型，我们可以修改隐藏层的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 4.616747553776374, prediction = [5.14780976e-04 3.24250638e-04 2.94601507e-05 9.50104717e-06]\n",
      "iteration 500, loss = 0.5710529566411006, prediction = [0.37210301 0.7456691  0.38136193 0.42956322]\n",
      "iteration 1000, loss = 0.45713826259043727, prediction = [0.3741349  0.71399129 0.55684197 0.35439945]\n",
      "iteration 1500, loss = 0.35316626696045617, prediction = [0.33216436 0.75522239 0.66319968 0.27205495]\n",
      "iteration 2000, loss = 0.26058237588171806, prediction = [0.26811827 0.80758732 0.74973902 0.20424104]\n",
      "iteration 2500, loss = 0.18860042139584576, prediction = [0.20553445 0.85458684 0.81671299 0.15186151]\n",
      "iteration 3000, loss = 0.13766359784902774, prediction = [0.15564704 0.89078918 0.86489066 0.11367364]\n",
      "iteration 3500, loss = 0.1029807124548807, prediction = [0.1192586  0.91677292 0.89827974 0.08676534]\n",
      "iteration 4000, loss = 0.07935282330571405, prediction = [0.09338716 0.93507938 0.9213334  0.06789923]\n",
      "iteration 4500, loss = 0.06292907503732903, prediction = [0.07489225 0.94808882 0.93750428 0.05448941]\n",
      "iteration 5000, loss = 0.0511977563403168, prediction = [0.06142356 0.9575225  0.9491231  0.04474524]\n",
      "iteration 5500, loss = 0.042579260797914555, prediction = [0.05139034 0.96452709 0.95769064 0.03749049]\n",
      "iteration 6000, loss = 0.03607845131347889, prediction = [0.04374389 0.96985157 0.96416766 0.03196011]\n",
      "iteration 6500, loss = 0.031057176057151175, prediction = [0.03779084 0.97398808 0.96917711 0.02765164]\n",
      "iteration 7000, loss = 0.027096377636745272, prediction = [0.03306582 0.97726544 0.97313122 0.02422898]\n",
      "iteration 7500, loss = 0.023913866022332975, prediction = [0.02925032 0.97990794 0.97630912 0.02146248]\n",
      "iteration 8000, loss = 0.02131490637526555, prediction = [0.02612175 0.98207185 0.97890418 0.01919177]\n",
      "iteration 8500, loss = 0.019161993659865893, prediction = [0.02352137 0.98386838 0.98105332 0.01730251]\n",
      "iteration 9000, loss = 0.017355958585721577, prediction = [0.02133376 0.98537821 0.98285548 0.01571156]\n",
      "iteration 9500, loss = 0.015823885874999926, prediction = [0.01947351 0.98666095 0.98438354 0.01435737]\n",
      "iteration 10000, loss = 0.01451121446647646, prediction = [0.01787634 0.98776139 0.98569206 0.01319359]\n",
      "iteration 10500, loss = 0.013376461128736878, prediction = [0.01649316 0.98871371 0.98682257 0.01218482]\n",
      "iteration 11000, loss = 0.012387625238072667, prediction = [0.01528594 0.98954432 0.9878071  0.0113036 ]\n",
      "iteration 11500, loss = 0.011519693125965659, prediction = [0.01422484 0.99027396 0.98867072 0.01052839]\n",
      "iteration 12000, loss = 0.010752875607673533, prediction = [0.0132862  0.99091903 0.98943326 0.00984208]\n",
      "iteration 12500, loss = 0.010071343303967131, prediction = [0.01245103 0.99149271 0.99011055 0.00923095]\n",
      "iteration 13000, loss = 0.009462305687668343, prediction = [0.01170396 0.99200563 0.99071543 0.00868387]\n",
      "iteration 13500, loss = 0.008915331228132012, prediction = [0.01103241 0.99246649 0.99125834 0.00819174]\n",
      "iteration 14000, loss = 0.008421839138183914, prediction = [0.01042604 0.99288246 0.99174786 0.00774705]\n",
      "iteration 14500, loss = 0.007974714932672301, prediction = [0.00987623 0.99325948 0.99219112 0.00734358]\n",
      "iteration 15000, loss = 0.00756801645648371, prediction = [0.00937579 0.99360253 0.99259407 0.00697609]\n",
      "iteration 15500, loss = 0.007196746804452625, prediction = [0.00891866 0.99391578 0.99296171 0.00664019]\n",
      "iteration 16000, loss = 0.006856677248768033, prediction = [0.00849971 0.99420278 0.99329827 0.00633216]\n",
      "iteration 16500, loss = 0.0065442079388232976, prediction = [0.00811455 0.99446655 0.99360734 0.00604881]\n",
      "iteration 17000, loss = 0.0062562574087330165, prediction = [0.00775945 0.99470968 0.99389201 0.00578742]\n",
      "iteration 17500, loss = 0.005990174255213593, prediction = [0.00743116 0.99493438 0.99415492 0.00554563]\n",
      "iteration 18000, loss = 0.005743666023448553, prediction = [0.0071269  0.9951426  0.99439837 0.00532142]\n",
      "iteration 18500, loss = 0.005514741556511173, prediction = [0.00684422 0.99533598 0.99462433 0.005113  ]\n",
      "iteration 19000, loss = 0.005301663958337266, prediction = [0.00658102 0.99551601 0.99483456 0.00491885]\n",
      "iteration 19500, loss = 0.005102911983190579, prediction = [0.00633543 0.99568396 0.99503056 0.00473759]\n",
      "iteration 20000, loss = 0.00491714816027608, prediction = [0.00610581 0.99584095 0.99521366 0.00456805]\n",
      "iteration 20500, loss = 0.004743192335916914, prediction = [0.00589072 0.99598798 0.99538505 0.00440916]\n",
      "iteration 21000, loss = 0.004579999599739903, prediction = [0.00568889 0.99612592 0.99554576 0.00425998]\n",
      "iteration 21500, loss = 0.0044266417787733195, prediction = [0.00549916 0.99625557 0.99569673 0.0041197 ]\n",
      "iteration 22000, loss = 0.004282291851036142, prediction = [0.00532053 0.99637761 0.99583876 0.00398757]\n",
      "iteration 22500, loss = 0.004146210760361523, prediction = [0.0051521  0.99649266 0.99597261 0.00386292]\n",
      "iteration 23000, loss = 0.0040177362158860565, prediction = [0.00499304 0.9966013  0.99609892 0.00374517]\n",
      "iteration 23500, loss = 0.003896273139571562, prediction = [0.00484263 0.99670401 0.99621829 0.00363377]\n",
      "iteration 24000, loss = 0.0037812854883241937, prediction = [0.00470021 0.99680125 0.99633126 0.00352825]\n",
      "iteration 24500, loss = 0.0036722892275196165, prediction = [0.00456518 0.99689343 0.9964383  0.00342817]\n",
      "iteration 25000, loss = 0.003568846272904459, prediction = [0.00443701 0.99698092 0.99653984 0.00333313]\n",
      "iteration 25500, loss = 0.0034705592501023926, prediction = [0.00431521 0.99706405 0.99663629 0.00324278]\n",
      "iteration 26000, loss = 0.0033770669469988563, prediction = [0.00419933 0.99714314 0.99672801 0.0031568 ]\n",
      "iteration 26500, loss = 0.0032880403553965393, prediction = [0.00408896 0.99721844 0.99681531 0.00307487]\n",
      "iteration 27000, loss = 0.0032031792155403334, prediction = [0.00398374 0.99729023 0.9968985  0.00299675]\n",
      "iteration 27500, loss = 0.003122208991187099, prediction = [0.00388334 0.99735872 0.99697785 0.00292217]\n",
      "iteration 28000, loss = 0.003044878214457992, prediction = [0.00378743 0.99742414 0.99705361 0.0028509 ]\n",
      "iteration 28500, loss = 0.0029709561492479303, prediction = [0.00369573 0.99748668 0.997126   0.00278275]\n",
      "iteration 29000, loss = 0.002900230729863483, prediction = [0.00360799 0.99754652 0.99719525 0.00271752]\n",
      "iteration 29500, loss = 0.002832506738118465, prediction = [0.00352397 0.99760382 0.99726153 0.00265502]\n",
      "iteration 30000, loss = 0.0027676041875877968, prediction = [0.00344343 0.99765873 0.99732503 0.00259511]\n",
      "iteration 30500, loss = 0.002705356888297832, prediction = [0.00336618 0.9977114  0.99738592 0.00253762]\n",
      "iteration 31000, loss = 0.002645611168969499, prediction = [0.00329202 0.99776195 0.99744435 0.00248242]\n",
      "iteration 31500, loss = 0.002588224737169566, prediction = [0.00322079 0.9978105  0.99750045 0.00242938]\n",
      "iteration 32000, loss = 0.0025330656604540523, prediction = [0.00315231 0.99785718 0.99755435 0.00237837]\n",
      "iteration 32500, loss = 0.0024800114539049, prediction = [0.00308644 0.99790207 0.99760619 0.0023293 ]\n",
      "iteration 33000, loss = 0.0024289482614240398, prediction = [0.00302304 0.99794528 0.99765607 0.00228205]\n",
      "iteration 33500, loss = 0.0023797701198274763, prediction = [0.00296197 0.99798689 0.99770409 0.00223653]\n",
      "iteration 34000, loss = 0.002332378296208557, prediction = [0.00290311 0.99802699 0.99775036 0.00219264]\n",
      "iteration 34500, loss = 0.00228668069026838, prediction = [0.00284636 0.99806566 0.99779496 0.00215031]\n",
      "iteration 35000, loss = 0.0022425912943590575, prediction = [0.00279159 0.99810297 0.99783798 0.00210945]\n",
      "iteration 35500, loss = 0.0022000297048937988, prediction = [0.00273872 0.99813899 0.9978795  0.00207   ]\n",
      "iteration 36000, loss = 0.0021589206795555307, prediction = [0.00268765 0.99817378 0.9979196  0.00203188]\n",
      "iteration 36500, loss = 0.002119193735412435, prediction = [0.00263829 0.9982074  0.99795833 0.00199504]\n",
      "iteration 37000, loss = 0.0020807827836321177, prediction = [0.00259057 0.9982399  0.99799578 0.0019594 ]\n",
      "iteration 37500, loss = 0.0020436257969942955, prediction = [0.0025444  0.99827134 0.99803199 0.00192491]\n",
      "iteration 38000, loss = 0.0020076645068427155, prediction = [0.00249971 0.99830178 0.99806703 0.00189152]\n",
      "iteration 38500, loss = 0.0019728441265044285, prediction = [0.00245643 0.99833124 0.99810095 0.00185919]\n",
      "iteration 39000, loss = 0.0019391130985361566, prediction = [0.00241451 0.99835979 0.9981338  0.00182785]\n",
      "iteration 39500, loss = 0.0019064228634573161, prediction = [0.00237388 0.99838745 0.99816563 0.00179748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 40000, loss = 0.0018747276478835405, prediction = [0.00233448 0.99841428 0.99819649 0.00176802]\n",
      "iteration 40500, loss = 0.0018439842702018038, prediction = [0.00229627 0.99844029 0.99822641 0.00173943]\n",
      "iteration 41000, loss = 0.0018141519621297023, prediction = [0.00225918 0.99846554 0.99825544 0.00171169]\n",
      "iteration 41500, loss = 0.0017851922046746762, prediction = [0.00222318 0.99849005 0.99828362 0.00168475]\n",
      "iteration 42000, loss = 0.0017570685771660722, prediction = [0.00218821 0.99851385 0.99831097 0.00165858]\n",
      "iteration 42500, loss = 0.001729746618168728, prediction = [0.00215424 0.99853697 0.99833755 0.00163315]\n",
      "iteration 43000, loss = 0.0017031936972106722, prediction = [0.00212122 0.99855944 0.99836336 0.00160843]\n",
      "iteration 43500, loss = 0.001677378896361662, prediction = [0.00208912 0.99858129 0.99838846 0.00158439]\n",
      "iteration 44000, loss = 0.001652272900799473, prediction = [0.0020579  0.99860254 0.99841286 0.00156101]\n",
      "iteration 44500, loss = 0.0016278478975823192, prediction = [0.00202753 0.99862321 0.99843659 0.00153825]\n",
      "iteration 45000, loss = 0.0016040774819247976, prediction = [0.00199796 0.99864332 0.99845969 0.0015161 ]\n",
      "iteration 45500, loss = 0.0015809365703396208, prediction = [0.00196918 0.99866291 0.99848217 0.00149453]\n",
      "iteration 46000, loss = 0.0015584013200711133, prediction = [0.00194115 0.99868198 0.99850405 0.00147352]\n",
      "iteration 46500, loss = 0.0015364490542979156, prediction = [0.00191385 0.99870056 0.99852537 0.00145305]\n",
      "iteration 47000, loss = 0.0015150581926315094, prediction = [0.00188724 0.99871866 0.99854614 0.0014331 ]\n",
      "iteration 47500, loss = 0.0014942081864838513, prediction = [0.00186131 0.9987363  0.99856638 0.00141365]\n",
      "iteration 48000, loss = 0.001473879458909374, prediction = [0.00183602 0.99875351 0.99858611 0.00139468]\n",
      "iteration 48500, loss = 0.0014540533485708975, prediction = [0.00181135 0.99877029 0.99860534 0.00137617]\n",
      "iteration 49000, loss = 0.0014347120575029064, prediction = [0.00178729 0.99878666 0.99862411 0.00135812]\n",
      "iteration 49500, loss = 0.0014158386023785325, prediction = [0.00176381 0.99880263 0.99864242 0.00134049]\n",
      "iteration 50000, loss = 0.0013974167690103047, prediction = [0.00174089 0.99881822 0.99866028 0.00132329]\n",
      "iteration 50500, loss = 0.0013794310698395946, prediction = [0.00171851 0.99883344 0.99867772 0.00130649]\n",
      "iteration 51000, loss = 0.0013618667041893457, prediction = [0.00169666 0.9988483  0.99869475 0.00129008]\n",
      "iteration 51500, loss = 0.0013447095210739359, prediction = [0.00167531 0.99886282 0.99871139 0.00127404]\n",
      "iteration 52000, loss = 0.0013279459843786399, prediction = [0.00165445 0.99887701 0.99872763 0.00125837]\n",
      "iteration 52500, loss = 0.0013115631402334917, prediction = [0.00163406 0.99889087 0.99874351 0.00124306]\n",
      "iteration 53000, loss = 0.0012955485864261898, prediction = [0.00161413 0.99890442 0.99875903 0.00122808]\n",
      "iteration 53500, loss = 0.001279890443704149, prediction = [0.00159465 0.99891768 0.9987742  0.00121344]\n",
      "iteration 54000, loss = 0.0012645773288353635, prediction = [0.00157559 0.99893063 0.99878903 0.00119912]\n",
      "iteration 54500, loss = 0.0012495983293020004, prediction = [0.00155695 0.99894331 0.99880354 0.0011851 ]\n",
      "iteration 55000, loss = 0.001234942979514415, prediction = [0.00153871 0.99895571 0.99881773 0.00117139]\n",
      "iteration 55500, loss = 0.001220601238440208, prediction = [0.00152086 0.99896785 0.99883161 0.00115797]\n",
      "iteration 56000, loss = 0.0012065634685522622, prediction = [0.00150338 0.99897973 0.9988452  0.00114483]\n",
      "iteration 56500, loss = 0.0011928204160058956, prediction = [0.00148628 0.99899136 0.9988585  0.00113196]\n",
      "iteration 57000, loss = 0.0011793631919634312, prediction = [0.00146953 0.99900275 0.99887153 0.00111936]\n",
      "iteration 57500, loss = 0.001166183254989515, prediction = [0.00145312 0.9990139  0.99888428 0.00110701]\n",
      "iteration 58000, loss = 0.0011532723944467438, prediction = [0.00143705 0.99902483 0.99889677 0.00109492]\n",
      "iteration 58500, loss = 0.0011406227148261397, prediction = [0.0014213  0.99903553 0.99890901 0.00108307]\n",
      "iteration 59000, loss = 0.0011282266209532055, prediction = [0.00140587 0.99904602 0.998921   0.00107145]\n",
      "iteration 59500, loss = 0.0011160768040113973, prediction = [0.00139075 0.9990563  0.99893275 0.00106007]\n",
      "iteration 60000, loss = 0.0011041662283329651, prediction = [0.00137592 0.99906638 0.99894427 0.0010489 ]\n",
      "iteration 60500, loss = 0.001092488118907957, prediction = [0.00136138 0.99907626 0.99895556 0.00103795]\n",
      "iteration 61000, loss = 0.00108103594956635, prediction = [0.00134712 0.99908595 0.99896663 0.00102722]\n",
      "iteration 61500, loss = 0.001069803431792114, prediction = [0.00133314 0.99909546 0.99897749 0.00101668]\n",
      "iteration 62000, loss = 0.001058784504130283, prediction = [0.00131942 0.99910478 0.99898814 0.00100635]\n",
      "iteration 62500, loss = 0.0010479733221507082, prediction = [1.30595921e-03 9.99113930e-01 9.98998585e-01 9.96205035e-04]\n",
      "iteration 63000, loss = 0.0010373642489348131, prediction = [1.29275004e-03 9.99122908e-01 9.99008836e-01 9.86251287e-04]\n",
      "iteration 63500, loss = 0.0010269518460546347, prediction = [1.27978547e-03 9.99131718e-01 9.99018896e-01 9.76480586e-04]\n",
      "iteration 64000, loss = 0.0010167308650139112, prediction = [1.26705898e-03 9.99140367e-01 9.99028769e-01 9.66888075e-04]\n",
      "iteration 64500, loss = 0.0010066962391254365, prediction = [1.25456427e-03 9.99148858e-01 9.99038461e-01 9.57469065e-04]\n",
      "iteration 65000, loss = 0.0009968430757969877, prediction = [1.24229526e-03 9.99157196e-01 9.99047977e-01 9.48219030e-04]\n",
      "iteration 65500, loss = 0.000987166649204565, prediction = [1.23024610e-03 9.99165384e-01 9.99057321e-01 9.39133595e-04]\n",
      "iteration 66000, loss = 0.000977662393328988, prediction = [1.21841109e-03 9.99173426e-01 9.99066498e-01 9.30208533e-04]\n",
      "iteration 66500, loss = 0.0009683258953355179, prediction = [1.20678476e-03 9.99181326e-01 9.99075512e-01 9.21439757e-04]\n",
      "iteration 67000, loss = 0.0009591528892775351, prediction = [1.19536181e-03 9.99189088e-01 9.99084366e-01 9.12823318e-04]\n",
      "iteration 67500, loss = 0.0009501392501054004, prediction = [1.18413711e-03 9.99196715e-01 9.99093066e-01 9.04355395e-04]\n",
      "iteration 68000, loss = 0.0009412809879637601, prediction = [1.17310570e-03 9.99204210e-01 9.99101615e-01 8.96032290e-04]\n",
      "iteration 68500, loss = 0.0009325742427616289, prediction = [1.16226279e-03 9.99211577e-01 9.99110017e-01 8.87850429e-04]\n",
      "iteration 69000, loss = 0.0009240152789994103, prediction = [1.15160374e-03 9.99218819e-01 9.99118275e-01 8.79806349e-04]\n",
      "iteration 69500, loss = 0.0009156004808400562, prediction = [1.14112405e-03 9.99225939e-01 9.99126393e-01 8.71896698e-04]\n",
      "iteration 70000, loss = 0.0009073263474097854, prediction = [1.13081937e-03 9.99232940e-01 9.99134375e-01 8.64118232e-04]\n",
      "iteration 70500, loss = 0.0008991894883173183, prediction = [1.12068548e-03 9.99239825e-01 9.99142223e-01 8.56467808e-04]\n",
      "iteration 71000, loss = 0.0008911866193790279, prediction = [1.11071831e-03 9.99246596e-01 9.99149941e-01 8.48942379e-04]\n",
      "iteration 71500, loss = 0.0008833145585392464, prediction = [1.10091390e-03 9.99253257e-01 9.99157532e-01 8.41538996e-04]\n",
      "iteration 72000, loss = 0.0008755702219764153, prediction = [1.09126841e-03 9.99259810e-01 9.99165000e-01 8.34254797e-04]\n",
      "iteration 72500, loss = 0.0008679506203837621, prediction = [1.08177814e-03 9.99266257e-01 9.99172346e-01 8.27087009e-04]\n",
      "iteration 73000, loss = 0.0008604528554167003, prediction = [1.07243947e-03 9.99272600e-01 9.99179574e-01 8.20032943e-04]\n",
      "iteration 73500, loss = 0.000853074116298086, prediction = [1.06324892e-03 9.99278844e-01 9.99186686e-01 8.13089991e-04]\n",
      "iteration 74000, loss = 0.000845811676572833, prediction = [1.05420309e-03 9.99284988e-01 9.99193686e-01 8.06255622e-04]\n",
      "iteration 74500, loss = 0.0008386628910051938, prediction = [1.04529869e-03 9.99291037e-01 9.99200575e-01 7.99527381e-04]\n",
      "iteration 75000, loss = 0.000831625192610239, prediction = [1.03653254e-03 9.99296991e-01 9.99207357e-01 7.92902885e-04]\n",
      "iteration 75500, loss = 0.0008246960898140668, prediction = [1.02790154e-03 9.99302854e-01 9.99214033e-01 7.86379820e-04]\n",
      "iteration 76000, loss = 0.0008178731637355857, prediction = [1.01940267e-03 9.99308627e-01 9.99220607e-01 7.79955941e-04]\n",
      "iteration 76500, loss = 0.0008111540655842935, prediction = [1.01103302e-03 9.99314311e-01 9.99227079e-01 7.73629064e-04]\n",
      "iteration 77000, loss = 0.0008045365141678121, prediction = [1.00278976e-03 9.99319910e-01 9.99233453e-01 7.67397073e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 77500, loss = 0.0007980182935044676, prediction = [9.94670122e-04 9.99325425e-01 9.99239731e-01 7.61257907e-04]\n",
      "iteration 78000, loss = 0.0007915972505354301, prediction = [9.86671434e-04 9.99330858e-01 9.99245915e-01 7.55209566e-04]\n",
      "iteration 78500, loss = 0.0007852712929319199, prediction = [9.78791095e-04 9.99336210e-01 9.99252007e-01 7.49250106e-04]\n",
      "iteration 79000, loss = 0.0007790383869923067, prediction = [9.71026574e-04 9.99341483e-01 9.99258008e-01 7.43377637e-04]\n",
      "iteration 79500, loss = 0.0007728965556261396, prediction = [9.63375414e-04 9.99346679e-01 9.99263921e-01 7.37590321e-04]\n",
      "iteration 80000, loss = 0.0007668438764187931, prediction = [9.55835223e-04 9.99351800e-01 9.99269748e-01 7.31886371e-04]\n",
      "iteration 80500, loss = 0.000760878479775882, prediction = [9.48403677e-04 9.99356847e-01 9.99275490e-01 7.26264049e-04]\n",
      "iteration 81000, loss = 0.000754998547141076, prediction = [9.41078514e-04 9.99361822e-01 9.99281149e-01 7.20721663e-04]\n",
      "iteration 81500, loss = 0.0007492023092855794, prediction = [9.33857534e-04 9.99366726e-01 9.99286728e-01 7.15257567e-04]\n",
      "iteration 82000, loss = 0.0007434880446646578, prediction = [9.26738595e-04 9.99371560e-01 9.99292227e-01 7.09870162e-04]\n",
      "iteration 82500, loss = 0.0007378540778402013, prediction = [9.19719614e-04 9.99376326e-01 9.99297648e-01 7.04557888e-04]\n",
      "iteration 83000, loss = 0.0007322987779639549, prediction = [9.12798562e-04 9.99381026e-01 9.99302993e-01 6.99319227e-04]\n",
      "iteration 83500, loss = 0.0007268205573196323, prediction = [9.05973465e-04 9.99385661e-01 9.99308263e-01 6.94152703e-04]\n",
      "iteration 84000, loss = 0.0007214178699226254, prediction = [8.99242400e-04 9.99390232e-01 9.99313460e-01 6.89056877e-04]\n",
      "iteration 84500, loss = 0.0007160892101722506, prediction = [8.92603493e-04 9.99394740e-01 9.99318586e-01 6.84030348e-04]\n",
      "iteration 85000, loss = 0.0007108331115560215, prediction = [8.86054921e-04 9.99399186e-01 9.99323641e-01 6.79071749e-04]\n",
      "iteration 85500, loss = 0.0007056481454033061, prediction = [8.79594907e-04 9.99403573e-01 9.99328627e-01 6.74179753e-04]\n",
      "iteration 86000, loss = 0.000700532919685195, prediction = [8.73221718e-04 9.99407900e-01 9.99333546e-01 6.69353061e-04]\n",
      "iteration 86500, loss = 0.0006954860778606589, prediction = [8.66933668e-04 9.99412170e-01 9.99338399e-01 6.64590412e-04]\n",
      "iteration 87000, loss = 0.0006905062977645454, prediction = [8.60729112e-04 9.99416382e-01 9.99343187e-01 6.59890574e-04]\n",
      "iteration 87500, loss = 0.0006855922905372718, prediction = [8.54606446e-04 9.99420539e-01 9.99347911e-01 6.55252346e-04]\n",
      "iteration 88000, loss = 0.0006807427995940159, prediction = [8.48564106e-04 9.99424642e-01 9.99352573e-01 6.50674559e-04]\n",
      "iteration 88500, loss = 0.0006759565996316205, prediction = [8.42600570e-04 9.99428691e-01 9.99357173e-01 6.46156069e-04]\n",
      "iteration 89000, loss = 0.0006712324956716548, prediction = [8.36714349e-04 9.99432687e-01 9.99361714e-01 6.41695765e-04]\n",
      "iteration 89500, loss = 0.0006665693221379883, prediction = [8.30903995e-04 9.99436632e-01 9.99366195e-01 6.37292560e-04]\n",
      "iteration 90000, loss = 0.0006619659419677351, prediction = [8.25168091e-04 9.99440526e-01 9.99370619e-01 6.32945394e-04]\n",
      "iteration 90500, loss = 0.0006574212457538435, prediction = [8.19505259e-04 9.99444371e-01 9.99374986e-01 6.28653234e-04]\n",
      "iteration 91000, loss = 0.0006529341509185784, prediction = [8.13914150e-04 9.99448167e-01 9.99379297e-01 6.24415069e-04]\n",
      "iteration 91500, loss = 0.000648503600915834, prediction = [8.08393450e-04 9.99451915e-01 9.99383554e-01 6.20229916e-04]\n",
      "iteration 92000, loss = 0.0006441285644614482, prediction = [8.02941877e-04 9.99455616e-01 9.99387757e-01 6.16096813e-04]\n",
      "iteration 92500, loss = 0.0006398080347912857, prediction = [7.97558177e-04 9.99459271e-01 9.99391907e-01 6.12014821e-04]\n",
      "iteration 93000, loss = 0.0006355410289440374, prediction = [7.92241128e-04 9.99462880e-01 9.99396006e-01 6.07983024e-04]\n",
      "iteration 93500, loss = 0.0006313265870693138, prediction = [7.86989534e-04 9.99466445e-01 9.99400053e-01 6.04000525e-04]\n",
      "iteration 94000, loss = 0.0006271637717598663, prediction = [7.81802230e-04 9.99469967e-01 9.99404051e-01 6.00066451e-04]\n",
      "iteration 94500, loss = 0.0006230516674062925, prediction = [7.76678076e-04 9.99473445e-01 9.99408000e-01 5.96179949e-04]\n",
      "iteration 95000, loss = 0.0006189893795735308, prediction = [7.71615959e-04 9.99476881e-01 9.99411900e-01 5.92340182e-04]\n",
      "iteration 95500, loss = 0.0006149760343990876, prediction = [7.66614792e-04 9.99480276e-01 9.99415754e-01 5.88546336e-04]\n",
      "iteration 96000, loss = 0.0006110107780110473, prediction = [7.61673512e-04 9.99483630e-01 9.99419561e-01 5.84797615e-04]\n",
      "iteration 96500, loss = 0.0006070927759655354, prediction = [7.56791081e-04 9.99486945e-01 9.99423322e-01 5.81093239e-04]\n",
      "iteration 97000, loss = 0.0006032212127031558, prediction = [7.51966483e-04 9.99490220e-01 9.99427038e-01 5.77432447e-04]\n",
      "iteration 97500, loss = 0.0005993952910235708, prediction = [7.47198727e-04 9.99493456e-01 9.99430711e-01 5.73814495e-04]\n",
      "iteration 98000, loss = 0.0005956142315769702, prediction = [7.42486842e-04 9.99496654e-01 9.99434340e-01 5.70238656e-04]\n",
      "iteration 98500, loss = 0.0005918772723724028, prediction = [7.37829881e-04 9.99499815e-01 9.99437926e-01 5.66704219e-04]\n",
      "iteration 99000, loss = 0.0005881836683024937, prediction = [7.33226916e-04 9.99502940e-01 9.99441470e-01 5.63210486e-04]\n",
      "iteration 99500, loss = 0.0005845326906829652, prediction = [7.28677040e-04 9.99506028e-01 9.99444974e-01 5.59756779e-04]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "\n",
    "n = x.shape[0]\n",
    "p = x.shape[1]\n",
    "d = y.shape[1]\n",
    "r = 10\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "nepoch = 100000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(nepoch):\n",
    "    z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "    phat = a2\n",
    "    loss = loss_function(y, phat)\n",
    "    dw1, db1, dw2, db2 = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss}, prediction = {phat.squeeze()}\")\n",
    "        \n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
